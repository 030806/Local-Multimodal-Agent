import os
import torch
import chromadb
from PIL import Image
from transformers import CLIPModel, CLIPProcessor
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from modules.config import DB_DIR, EMBEDDING_MODEL_PATH, CLIP_MODEL_PATH


class VectorDBManager:
    def __init__(self):
        # 1. æ£€æŸ¥å¹¶è®¾ç½®è®¾å¤‡ (GPU/CPU)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.device}")

        # 2. åˆå§‹åŒ–æ–‡çŒ® Embedding æ¨¡å‹ (çº¯æ–‡æœ¬)
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ–‡æ¡£ Embedding æ¨¡å‹: {os.path.basename(EMBEDDING_MODEL_PATH)}...")
        self.doc_embedder = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_PATH)

        # 3. åˆå§‹åŒ–åŸç”Ÿ CLIP æ¨¡å‹ (å¤šæ¨¡æ€)
        print(f"ğŸ”„ æ­£åœ¨é€šè¿‡ Transformers åŠ è½½ CLIP æ¨¡å‹...")

        self.clip_model = CLIPModel.from_pretrained(CLIP_MODEL_PATH).to(self.device)
        # self.clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_PATH)
        self.clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_PATH, use_fast=True)

        # 4. åˆå§‹åŒ– ChromaDB
        self.client = chromadb.PersistentClient(path=DB_DIR)

        # æ–‡æ¡£ Collection
        self.paper_db = Chroma(
            client=self.client,
            collection_name="paper_collection",
            embedding_function=self.doc_embedder
        )

        # å›¾åƒ Collection
        self.image_col = self.client.get_or_create_collection(name="image_collection")

    # ================= æ™ºèƒ½å›¾åƒç®¡ç†æ¨¡å— (2.2) =================

    def add_image(self, img_path):
        """ç”Ÿæˆå›¾åƒ Embedding å¹¶å­˜å…¥åº“"""
        try:
            image = Image.open(img_path).convert("RGB")

            # ä½¿ç”¨ CLIPProcessor é¢„å¤„ç†å›¾åƒå¹¶ç”Ÿæˆ Embedding
            with torch.no_grad():
                inputs = self.clip_processor(images=image, return_tensors="pt").to(self.device)
                image_features = self.clip_model.get_image_features(**inputs)
                # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
                image_features /= image_features.norm(dim=-1, keepdim=True)
                img_embedding = image_features.cpu().numpy().flatten().tolist()

            self.image_col.add(
                embeddings=[img_embedding],
                documents=[img_path],
                metadatas=[{"file_path": img_path}],
                ids=[os.path.basename(img_path)]
            )
            return True
        except Exception as e:
            print(f"âŒ å›¾ç‰‡å¤„ç†å¤±è´¥ {img_path}: {e}")
            return False

    def search_images(self, query_text, k=3):
        """ä»¥æ–‡æœå›¾ï¼šå¸¦æœ‰ Prompt Template ä¼˜åŒ–çš„æ£€ç´¢"""
        try:
            # 1. ä¼˜åŒ–æç¤ºè¯ï¼šå¦‚æœç”¨æˆ·æ²¡è¾“å…¥ a photo ofï¼Œæˆ‘ä»¬è‡ªåŠ¨è¡¥ä¸Š
            # è¿™æ ·å¯ä»¥æ›´å¥½åœ°æ¿€æ´» CLIP åœ¨é¢„è®­ç»ƒæ—¶å­¦åˆ°çš„è§†è§‰ç‰¹å¾
            if not query_text.lower().startswith("a photo of"):
                optimized_query = f"a photo of a {query_text}"
            else:
                optimized_query = query_text

            print(f"ğŸª„ ä¼˜åŒ–åçš„ Query: '{optimized_query}'")

            with torch.no_grad():
                # ä½¿ç”¨ CLIPProcessor å¤„ç†ä¼˜åŒ–åçš„æœç´¢æ–‡æœ¬
                inputs = self.clip_processor(
                    text=[optimized_query],
                    return_tensors="pt",
                    padding=True
                ).to(self.device)

                text_features = self.clip_model.get_text_features(**inputs)
                # å½’ä¸€åŒ–
                text_features /= text_features.norm(dim=-1, keepdim=True)
                query_embedding = text_features.cpu().numpy().flatten().tolist()

            results = self.image_col.query(
                query_embeddings=[query_embedding],
                n_results=k
            )

            formatted_results = []
            if results['documents']:
                for i in range(len(results['documents'][0])):
                    formatted_results.append({
                        "path": results['documents'][0][i],
                        "score": results['distances'][0][i]
                    })
            return formatted_results
        except Exception as e:
            print(f"âŒ å›¾åƒæ£€ç´¢å¤±è´¥: {e}")
            return []
    # def search_images(self, query_text, k=3):
    #     """ä»¥æ–‡æœå›¾ï¼šé€šè¿‡ CLIP æ–‡æœ¬åˆ†æ”¯æ£€ç´¢å›¾åƒ"""
    #     try:
    #         with torch.no_grad():
    #             # ä½¿ç”¨ CLIPProcessor å¤„ç†æœç´¢æ–‡æœ¬
    #             inputs = self.clip_processor(text=[query_text], return_tensors="pt", padding=True).to(self.device)
    #             text_features = self.clip_model.get_text_features(**inputs)
    #             # å½’ä¸€åŒ–
    #             text_features /= text_features.norm(dim=-1, keepdim=True)
    #             query_embedding = text_features.cpu().numpy().flatten().tolist()
    #
    #         results = self.image_col.query(
    #             query_embeddings=[query_embedding],
    #             n_results=k
    #         )
    #
    #         formatted_results = []
    #         if results['documents']:
    #             for i in range(len(results['documents'][0])):
    #                 formatted_results.append({
    #                     "path": results['documents'][0][i],
    #                     "score": results['distances'][0][i]
    #                 })
    #
    #         return formatted_results
    #     except Exception as e:
    #         print(f"âŒ å›¾åƒæ£€ç´¢å¤±è´¥: {e}")
    #         return []

    # ================= æ–‡çŒ®ç®¡ç†æ¨¡å— (2.1) =================

    def add_documents(self, documents):
        """å°† PDF åˆ‡ç‰‡å­˜å…¥æ–‡æ¡£åº“"""
        self.paper_db.add_documents(documents)
        print(f"âœ… å·²å°† {len(documents)} ä¸ªæ–‡çŒ®ç‰‡æ®µå­˜å…¥æ•°æ®åº“ã€‚")

    def search_papers(self, query, k=3):
        """è¯­ä¹‰æœç´¢æ–‡çŒ®"""
        return self.paper_db.similarity_search(query, k=k)