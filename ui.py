import streamlit as st
import os
from PIL import Image
from modules.vector_store import VectorDBManager
from modules.classifier import SemanticClassifier
from modules.doc_processor import DocumentProcessor

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="Local Multimodal AI Agent", page_icon="ğŸ¤–", layout="wide")


# åˆå§‹åŒ–åç«¯ç»„ä»¶ (ä½¿ç”¨ st.cache_resource é¿å…é‡å¤åŠ è½½æ¨¡å‹)
@st.cache_resource
def get_managers():
    return VectorDBManager(), SemanticClassifier(), DocumentProcessor()


db_manager, classifier, doc_processor = get_managers()

# --- ä¾§è¾¹æ å¯¼èˆª ---
st.sidebar.title("ğŸ¤– å¯¼èˆªæ§åˆ¶å°")
menu = st.sidebar.radio("é€‰æ‹©åŠŸèƒ½æ¨¡å—", [
    "ğŸ  é¦–é¡µ",
    "ğŸ“„ æ–‡çŒ®ä¸Šä¼ ä¸æ•´ç†",
    "ğŸ“‚ æ‰¹é‡è®ºæ–‡æ•´ç†",
    "ğŸ” æ–‡çŒ®è¯­ä¹‰æœç´¢",
    "ğŸ–¼ï¸ å›¾åƒåº“æœç´¢"
])

st.sidebar.markdown("---")
st.sidebar.info("é¡¹ç›®çŠ¶æ€ï¼šå·²è¿æ¥æœ¬åœ° CLIP & MiniLM æ¨¡å‹")

# --- 1. é¦–é¡µ ---
if menu == "ğŸ  é¦–é¡µ":
    st.title("æ¬¢è¿ä½¿ç”¨æœ¬åœ° AI æ™ºèƒ½ç®¡ç†åŠ©æ‰‹")
    st.markdown("""
    æœ¬é¡¹ç›®åˆ©ç”¨å¤šæ¨¡æ€ç¥ç»ç½‘ç»œæŠ€æœ¯ï¼Œä¸ºæ‚¨æä¾›ï¼š
    - **æ™ºèƒ½æ–‡çŒ®ç®¡ç†**ï¼šè‡ªåŠ¨åˆ†æ PDF ä¸»é¢˜å¹¶å½’æ¡£ï¼Œæ”¯æŒå…¨æ–‡è¯­ä¹‰æœç´¢ã€‚
    - **æ™ºèƒ½å›¾åƒç®¡ç†**ï¼šåˆ©ç”¨ CLIP æ¨¡å‹ï¼Œå®ç°â€œä»¥æ–‡æœå›¾â€ã€‚
    """)

    col1, col2 = st.columns(2)
    with col1:
        st.metric("æ–‡æ¡£ç´¢å¼•æ•°", "å·²å°±ç»ª")
    with col2:
        st.metric("å›¾åƒç´¢å¼•æ•°", "å·²å°±ç»ª")

# --- 2. æ–‡çŒ®ä¸Šä¼ ä¸æ•´ç† ---
elif menu == "ğŸ“„ æ–‡çŒ®ä¸Šä¼ ä¸æ•´ç†":
    st.header("ğŸ“„ ä¸Šä¼ æ–°è®ºæ–‡")

    uploaded_file = st.file_uploader("é€‰æ‹© PDF æ–‡ä»¶", type="pdf")
    topics_input = st.text_input("å®šä¹‰åˆ†ç±»ä¸»é¢˜ (é€—å·åˆ†éš”)", "NLP, Computer Vision, Reinforcement Learning,Deep Learning")

    if st.button("å¼€å§‹å¤„ç†å¹¶å½’ç±»"):
        if uploaded_file and topics_input:
            with st.spinner("ğŸš€ æ­£åœ¨æå–æ–‡æœ¬å¹¶è¿›è¡Œè¯­ä¹‰åˆ†ç±»..."):
                # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ä»¥ä¾¿å¤„ç†
                temp_path = os.path.join("test_data/papers", uploaded_file.name)
                with open(temp_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())

                topics = [t.strip() for t in topics_input.split(",")]

                # æ‰§è¡Œåç«¯é€»è¾‘
                splits, first_page_text = doc_processor.load_and_split(temp_path)
                category = classifier.classify_paper(first_page_text, topics)

                # ç§»åŠ¨å¹¶æ›´æ–°æ•°æ®åº“
                new_path = doc_processor.move_file(temp_path, category)
                for split in splits:
                    split.metadata['source'] = new_path
                    split.metadata['category'] = category
                db_manager.add_documents(splits)

                st.success(f"âœ… æ–‡ä»¶å·²è‡ªåŠ¨å½’ç±»è‡³: **[{category}]**")
                st.balloons()
        else:
            st.warning("è¯·ä¸Šä¼ æ–‡ä»¶å¹¶è¾“å…¥ä¸»é¢˜ã€‚")

# --- 3. æ–‡çŒ®è¯­ä¹‰æœç´¢ ---
elif menu == "ğŸ” æ–‡çŒ®è¯­ä¹‰æœç´¢":
    st.header("ğŸ” æ–‡çŒ®æ·±åº¦æœç´¢")
    query = st.text_input("è¾“å…¥æ‚¨çš„ç–‘é—® (ä¾‹å¦‚: How does attention mechanism work?)")
    index_only = st.checkbox("ä»…è¿”å›æ–‡ä»¶ç´¢å¼•")

    if st.button("æœç´¢"):
        if query:
            k = 10 if index_only else 3
            results = db_manager.search_papers(query, k=k)

            if results:
                if index_only:
                    seen = set()
                    for doc in results:
                        path = doc.metadata.get('source', 'Unknown')
                        if path not in seen:
                            st.write(f"ğŸ“„ **{os.path.basename(path)}**")
                            st.caption(f"è·¯å¾„: {path}")
                            seen.add(path)
                else:
                    for i, doc in enumerate(results):
                        with st.expander(
                                f"ç»“æœ {i + 1}: {os.path.basename(doc.metadata.get('source', ''))} (ç¬¬ {doc.metadata.get('page', 0) + 1} é¡µ)"):
                            st.write(f"**åˆ†ç±»æ ‡ç­¾:** :blue[{doc.metadata.get('category', 'N/A')}]")
                            st.write(f"**ç‰‡æ®µå†…å®¹:** ...{doc.page_content}...")
            else:
                st.error("æœªæ‰¾åˆ°åŒ¹é…å†…å®¹ã€‚")

# --- 4. å›¾åƒåº“æœç´¢ ---
elif menu == "ğŸ–¼ï¸ å›¾åƒåº“æœç´¢":
    st.header("ğŸ–¼ï¸ æ™ºèƒ½å›¾åƒç®¡ç†")

    # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šç´¢å¼•æ„å»º (Indexing) ---
    with st.expander("ğŸ› ï¸ å›¾åƒç´¢å¼•ç»´æŠ¤", expanded=False):
        st.write("å¦‚æœè¿™æ˜¯æ‚¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æˆ–æ›´æ¢äº†å›¾ç‰‡ç›®å½•ï¼Œè¯·å…ˆè¿›è¡Œç´¢å¼•ã€‚")
        img_dir = st.text_input("å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„", value="./test_data/images")

        if st.button("å¼€å§‹æ„å»º/æ›´æ–°å›¾åƒç´¢å¼•"):
            if os.path.exists(img_dir):
                image_extensions = ('.jpg', '.jpeg', '.png', '.bmp')
                files = [f for f in os.listdir(img_dir) if f.lower().endswith(image_extensions)]

                if not files:
                    st.warning("è¯¥ç›®å½•ä¸‹æ²¡æœ‰å‘ç°å›¾ç‰‡æ–‡ä»¶ã€‚")
                else:
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    count = 0

                    for i, filename in enumerate(files):
                        full_path = os.path.join(img_dir, filename)
                        status_text.text(f"æ­£åœ¨ç´¢å¼•: {filename}")
                        if db_manager.add_image(full_path):
                            count += 1
                        progress_bar.progress((i + 1) / len(files))

                    st.success(f"âœ¨ ç´¢å¼•å®Œæˆï¼å·²æˆåŠŸç´¢å¼• {count} å¼ å›¾ç‰‡ã€‚")
            else:
                st.error("è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ã€‚")

    st.markdown("---")

    # --- ç¬¬äºŒéƒ¨åˆ†ï¼šä»¥æ–‡æœå›¾ (Search) ---
    st.subheader("ğŸ” ä»¥æ–‡æœå›¾ (CLIP Search)")
    img_query = st.text_input("è¾“å…¥æè¿°è¯ (ä¾‹å¦‚: a photo of a dog, sunset, paper chart)")
    top_k = st.slider("è¿”å›ç»“æœæ•°é‡", 1, 10, 3)

    if st.button("æœç´¢å›¾ç‰‡"):
        if img_query:
            with st.spinner("ğŸ§  CLIP æ­£åœ¨ç†è§£è¯­ä¹‰..."):
                results = db_manager.search_images(img_query, k=top_k)

            if results:
                st.write(f"ä¸ºæ‚¨æ‰¾åˆ°ä»¥ä¸‹ {len(results)} å¼ æœ€åŒ¹é…çš„å›¾ç‰‡ï¼š")
                cols = st.columns(3)
                for idx, res in enumerate(results):
                    with cols[idx % 3]:
                        similarity = max(0, 1 - (res['score'] / 2.0)) * 100
                        # å°† use_column_width=True æ›¿æ¢ä¸º use_container_width=True
                        st.image(res['path'], use_container_width=True)
                        st.caption(f"ğŸ¯ åŒ¹é…åº¦: {similarity:.2f}%")
                        st.caption(f"ğŸ“‚ `{os.path.basename(res['path'])}`")
            else:
                st.info("ğŸ’¡ æœªæ‰¾åˆ°åŒ¹é…å›¾ç‰‡ã€‚è¯·ç¡®ä¿å·²å…ˆæ‰§è¡Œä¸Šæ–¹â€˜ç´¢å¼•ç»´æŠ¤â€™åŠŸèƒ½ã€‚")
elif menu == "ğŸ“‚ æ‰¹é‡è®ºæ–‡æ•´ç†":
    st.header("ğŸ“‚ ä¸€é”®æ•´ç†è®ºæ–‡æ–‡ä»¶å¤¹")
    st.info("ç³»ç»Ÿå°†æ‰«ææŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ PDFï¼Œè‡ªåŠ¨è¿›è¡Œè¯­ä¹‰åˆ†ç±»ã€ç§»åŠ¨æ–‡ä»¶å¹¶å»ºç«‹ç´¢å¼•ã€‚")

    source_dir = st.text_input("è¯·è¾“å…¥å¾…æ•´ç†çš„æ–‡ä»¶å¤¹è·¯å¾„ (ä¾‹å¦‚: ./test_data/raw_papers)")
    batch_topics = st.text_input("åˆ†ç±»ä¸»é¢˜ (é€—å·åˆ†éš”)", "NLP, Computer Vision, Reinforcement Learning,Deep Learning")

    if st.button("å¼€å§‹æ‰¹é‡æ•´ç†"):
        if not os.path.exists(source_dir):
            st.error("âŒ è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥åé‡è¯•ã€‚")
        else:
            # è·å–æ‰€æœ‰å¾…å¤„ç†çš„ PDF
            pdf_files = [f for f in os.listdir(source_dir) if f.lower().endswith('.pdf')]

            if not pdf_files:
                st.warning("æŸ¥æ—  PDF æ–‡ä»¶ã€‚")
            else:
                st.write(f"ğŸ” å‘ç° {len(pdf_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶...")

                # åˆå§‹åŒ–è¿›åº¦æ¡å’Œæ—¥å¿—å ä½ç¬¦
                progress_bar = st.progress(0)
                status_text = st.empty()
                log_area = st.expander("è¯¦ç»†å¤„ç†æ—¥å¿—", expanded=True)

                topics = [t.strip() for t in batch_topics.split(",")]
                success_count = 0

                for i, filename in enumerate(pdf_files):
                    file_path = os.path.join(source_dir, filename)
                    status_text.text(f"æ­£åœ¨å¤„ç† ({i + 1}/{len(pdf_files)}): {filename}")

                    try:
                        # 1. åŠ è½½ä¸åˆ‡ç‰‡
                        splits, first_page_text = doc_processor.load_and_split(file_path)

                        # 2. è¯­ä¹‰åˆ†ç±»
                        category = classifier.classify_paper(first_page_text, topics)

                        # 3. ç§»åŠ¨æ–‡ä»¶
                        new_path = doc_processor.move_file(file_path, category)

                        # 4. å­˜å…¥æ•°æ®åº“
                        for split in splits:
                            split.metadata['source'] = new_path
                            split.metadata['category'] = category
                        db_manager.add_documents(splits)

                        log_area.write(f"âœ… {filename} -> **[{category}]**")
                        success_count += 1

                    except Exception as e:
                        log_area.error(f"âŒ {filename} å¤„ç†å¤±è´¥: {str(e)}")

                    # æ›´æ–°è¿›åº¦æ¡
                    progress_bar.progress((i + 1) / len(pdf_files))

                st.success(f"âœ¨ æ‰¹é‡æ•´ç†å®Œæˆï¼æˆåŠŸå¤„ç† {success_count} ä¸ªæ–‡ä»¶ã€‚")
                st.balloons()