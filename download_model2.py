# download_models.py
import os
from modelscope import snapshot_download
from huggingface_hub import snapshot_download as hf_snapshot_download

# è®¾ç½®æ¨¡å‹ä¿å­˜çš„æ ¹ç›®å½• (ä½ å¯ä»¥æ”¹æˆä½ è‡ªå·±çš„è·¯å¾„ï¼Œä¾‹å¦‚ '/data/ljf/LLM/models/...')
CACHE_DIR = './agent/models'
print(f"ğŸš€ å‡†å¤‡ä¸‹è½½æ¨¡å‹åˆ°: {CACHE_DIR}")

# 1. ä¸‹è½½ Embedding æ¨¡å‹ (ç”¨äºæ–‡æ¡£æœç´¢)
# å¯¹åº” HuggingFace çš„ sentence-transformers/all-MiniLM-L6-v2

print("æ­£åœ¨ä» ModelScope ä¸‹è½½ CLIP æ¨¡å‹...")
# ä¸‹è½½ OpenAI å¼€æºçš„ CLIP ViT-B-32
# clip_path = snapshot_download(
#     'openai/clip-vit-base-patch32',
#     cache_dir=CACHE_DIR
# )
clip_path = hf_snapshot_download(
    repo_id='openai/clip-vit-base-patch32',
    cache_dir=CACHE_DIR,
    resume_download=True  # æ”¯æŒæ–­ç‚¹ç»­ä¼ 
)
print(f"âœ… CLIP æ¨¡å‹å·²ä¸‹è½½è‡³: {clip_path}")

print("æ­£åœ¨ä¸‹è½½ Embedding æ¨¡å‹ (all-MiniLM-L6-v2)...")
embedding_path = snapshot_download(
    'AI-ModelScope/all-MiniLM-L6-v2',
    cache_dir=CACHE_DIR
)
print(f"âœ… Embedding æ¨¡å‹å·²ä¸‹è½½: {embedding_path}")

# 2. (å¯é€‰) å¦‚æœä½ ä¹Ÿæƒ³ä¸‹è½½ DeepSeek çš„åŸå§‹æƒé‡ (é Ollama ç‰ˆ)
# å¦‚æœä½ å·²ç»ç”¨ Ollama è·‘èµ·æ¥äº†ï¼Œè¿™ä¸€æ­¥å¯ä»¥è·³è¿‡
# print("æ­£åœ¨ä¸‹è½½ DeepSeek æ¨¡å‹...")
# llm_path = snapshot_download(
#     'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',
#     cache_dir=CACHE_DIR
# )
# print(f"âœ… LLM æ¨¡å‹å·²ä¸‹è½½: {llm_path}")

